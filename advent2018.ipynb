{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advent of Code 2018\n",
    "\n",
    "Hey there! For this year of Advent of Code, I'm going to be trying to journal my progress through the month's problems using a Python notebook, inspired by the similar notebooks of [Peter Norvig](https://github.com/norvig/pytudes). I've considered doing other things this year, like perhaps using the Advent of Code as an excuse to learn a new language, but last year I only completed the first 8, so I figured I should focus on first trying to complete all of this year's problems before getting ahead of myself. (Plus, the easier challenges make for good interview problem practice, so it fits in with my current goals).\n",
    "\n",
    "Since I'm very comfortable with Python, I'm going to try to be competitive with timing (as the time to write code is the main bottleneck typically - not the program execution), so I will try to finish on the leaderboard. But this is my first time using an Python notebook, so there's a chance this will slow me down. Better hope for the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 0: Imports and utilities\n",
    "\n",
    "Following the inspiration of Peter Norvig, here I will try to put some code and imports that I think could be useful. For now I'll just include libraries and functions I've used a decent amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "from blist import blist\n",
    "from collections import Counter, defaultdict\n",
    "from heapq import heappush, heappop\n",
    "from itertools import accumulate, combinations, cycle, takewhile\n",
    "\n",
    "# source: https://stackoverflow.com/a/30558049\n",
    "def unique_permutations(elements):\n",
    "    if len(elements) == 1:\n",
    "        yield (elements[0],)\n",
    "    else:\n",
    "        unique_elements = set(elements)\n",
    "        for first_element in unique_elements:\n",
    "            remaining_elements = list(elements)\n",
    "            remaining_elements.remove(first_element)\n",
    "            for sub_permutation in unique_permutations(remaining_elements):\n",
    "                yield (first_element,) + sub_permutation\n",
    "\n",
    "def flatten(lst):\n",
    "    return [elem for sublst in lst for elem in sublst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Day 1](https://adventofcode.com/2018/day/1): Chronal Calibration\n",
    "\n",
    "This problem was very straight forward. Getting a tight time is just a matter of having your environment setup, and fortunately I don't think using jupyter really slowed me down much, as I already had prepared some starter code in advance for reading in input.\n",
    "\n",
    "Unfortunately, I took 1:48 to solve the first star, but to get in the top 100 I would have needed 1:32; and I took 6:32 on the second star, but to get in the top 100 I would have needed 5:28. Besides reading the problem as a bottleneck, during the second problem I was planning to loop over indices but ended up looping over values, so I ended up with an incorrect answer costing me a minute. Also, my code definitely looks pretty sloppy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first star\n",
    "\n",
    "with open('input/input1.txt') as f:\n",
    "    X = list(map(int, f.read().split()))\n",
    "\n",
    "sum(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n"
     ]
    }
   ],
   "source": [
    "# second star\n",
    "\n",
    "freqs = set()\n",
    "curr = 0\n",
    "while True:\n",
    "    found = False\n",
    "    for x in X:\n",
    "        curr += x\n",
    "        if curr in freqs:\n",
    "            found = True\n",
    "            print(curr)\n",
    "            break\n",
    "        freqs.add(curr)\n",
    "    if found:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how I would refactor the second star with a few extra minutes to spare. The interesting thing about this kind of problem is that it seems simple enough that it should be doable with a Python one-or-two-liner, but I don't have an exact intuition for it (though I'm sure someone on Reddit has done it). That said, I think for most people this type of problem feels easier to solve this with iterative methods than functional programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n"
     ]
    }
   ],
   "source": [
    "# second star: revision 1\n",
    "\n",
    "seen = set()\n",
    "for x in accumulate(cycle(X)):\n",
    "    if x in seen:\n",
    "        print(x)\n",
    "        break\n",
    "    seen.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second star: revision 2\n",
    "\n",
    "seen = set()\n",
    "def check_repeats(val):\n",
    "    if val not in seen:\n",
    "        seen.add(val)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "next(filter(check_repeats, accumulate(cycle(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Day 2](https://adventofcode.com/2018/day/2): Inventory Management System\n",
    "\n",
    "Unfortunately, I didn't have this time to do this challenge at midnight, so I can't really comment on speed for these questions, though they were only slightly more involved than Day 1. The first star is very easily solvable in Python using the Counter library function from the collections module - it's useful in quite a number of coding interview puzzles, in my experience.\n",
    "\n",
    "The second star involves a bit more work, as it seems to necessitate comparing all pairs of strings against one another to decide which differs exactly by one character. Both parts, calculating the number of differences, and deriving the string in common, can be simplified as Python one-liners without adding much unnecessary overhead. Writing Python one-liners tend to just feel satisfying to derive and look clean to look at, even though they don't always reflect how a person came up with the line.\n",
    "\n",
    "It's worth noting how out of habit (from websites like HackerRank and LeetCode), I tend to code defensively, doing things like iterating only through the minimum of the lengths of the two strings, even though all strings in the given input are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7192"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first star\n",
    "\n",
    "with open('input/input2.txt') as f:\n",
    "    X = f.read().split()\n",
    "\n",
    "twos, threes = 0, 0\n",
    "for x in X:\n",
    "    count = Counter(x)\n",
    "    if 2 in count.values():\n",
    "        twos += 1\n",
    "    if 3 in count.values():\n",
    "        threes += 1\n",
    "twos * threes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mbruvapghxlzycbhmfqjonsie'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second star\n",
    "\n",
    "def num_differences(s1, s2):\n",
    "    return sum([s1[i] != s2[i] for i in range(min(len(s1), len(s2)))])\n",
    "\n",
    "def in_common(s1, s2):\n",
    "    return ''.join([s1[i] for i in range(min(len(s1), len(s2))) if s1[i] == s2[i]])\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(i + 1, len(X)):\n",
    "        if num_differences(X[i], X[j]) == 1:\n",
    "            return in_common(X[i], X[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mbruvapghxlzycbhmfqjonsie'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second star: revision (using zip, combinations)\n",
    "\n",
    "def num_differences(s1, s2):\n",
    "    return sum([c1 != c2 for c1, c2 in zip(s1, s2)])\n",
    "\n",
    "def in_common(s1, s2):\n",
    "    return ''.join([c1 for c1, c2 in zip(s1, s2) if c1 == c2])\n",
    "\n",
    "in_common(*next(filter(lambda x: num_differences(x[0], x[1]) == 1, combinations(X, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Day 3](https://adventofcode.com/2018/day/3): No Matter How You Slice It\n",
    "\n",
    "That was satisfying for sure! I ended up solving this puzzle using a simple brute force approach, storing all of the positions in a grid and then performing the necessary operations to update the grid with appropriate ownership of the squares. For the second star, my first solution made the answer relatively easy to retrieve, since it just required me to iterate over in the same fashion and re-check the ownership of various regions.\n",
    "\n",
    "Unfortunately, the main part I got stuck on was something I should probably be fluent with: input parsing. I knew how to use string.split() to separate the lines, but the complex format of the output made regex a clear candidate for the problem. That said, I haven't necessarily used regex a lot, so I was originally looking up ways to separate strings by multiple separators in Python, before deciding that just gathering all of the sequences of digits would be more efficient.\n",
    "\n",
    "The other slight slowdown was that at the very end I remembered I had to flatten the list (or at least flattening would make for the cleanest solution), and I hadn't written a function for it in advance. Once these issues were figured out though, most of the coding was smooth sailing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113966"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first star\n",
    "\n",
    "with open('input/input3.txt') as f:\n",
    "    X = f.read().split('\\n')[:-1]  # remove the last line, which is empty\n",
    "\n",
    "grid = [[None] * 1000 for _ in range(1000)]\n",
    "for entry in X:\n",
    "    claim, x, y, w, h = map(int, re.findall('\\d+', entry))\n",
    "    for row in range(y, y + h):\n",
    "        for col in range(x, x + w):\n",
    "            if grid[row][col] is None:\n",
    "                grid[row][col] = claim\n",
    "            else:\n",
    "                grid[row][col] = 'X'\n",
    "\n",
    "sum([claim == 'X' for claim in flatten(grid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "# second star\n",
    "\n",
    "for entry in X:\n",
    "    claim, x, y, w, h = map(int, re.findall('\\d+', entry))\n",
    "    safe = True\n",
    "    for row in range(y, y + h):\n",
    "        for col in range(x, x + w):\n",
    "            if grid[row][col] == 'X':\n",
    "                safe = False\n",
    "                break\n",
    "        if not safe:\n",
    "            break\n",
    "    if safe:\n",
    "        print(claim)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "# second star: revision\n",
    "\n",
    "for entry in X:\n",
    "    claim, x, y, w, h = map(int, re.findall('\\d+', entry))\n",
    "    tiles = [item for row in grid[y:y + h] for item in row[x:x + w]]\n",
    "    if all(map(lambda x: x != 'X', tiles)):\n",
    "        print(claim)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Day 4](https://adventofcode.com/2018/day/4): Repose Record\n",
    "\n",
    "This update is quite delayed, but as I became preoccupied with final exams, I had to take a temporary hiatus on Advent of Code. But now that I am on holiday break, there is more time to complete these challenges (though I won't be competing for leaderboard positions). Anyway, onward to the puzzle!\n",
    "\n",
    "As with Day 3, I was slowed down a bit by first figuring out what the most efficient way to parse the data was. I used regular expressions again this time, but instead of splitting the string, I just used re.match while specifying groups in my pattern (using parentheses) so I could extract individual groups into tuples. I'm not sure if this is necessarily the most efficient method, but it's fairly readable and understandable, so I think it works fine. Since the time stamps of each string are fixed in size, it now occurs to me that I could probably have just picked slices out of the strings for the times, but I would still have to search for the guard IDs on the \"begins shift\" records.\n",
    "\n",
    "The rest of the calculation itself was fairly simple. Once I extracted the data into tuples (ordered by month, day, hour, minute), sorting in Python automatically places all of the records in the right order. Then I stored individual data for guards using a dictionary, associating an array of 60 integers for each guard. Once I determined the time interval during which a guard was sleeping, I simply incremented the corresponding entries of the array. It's possible this could be handled more efficiently somehow (perhaps with some variant of range queries?) but for a list of size 60, we can sleep safely at night. After we have the sleeping data for each guard, calculating the most-slept minutes (and the total time they spent sleeping) was easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39698"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first star\n",
    "\n",
    "with open('input/input4.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "records = []\n",
    "for line in lines:\n",
    "    m = re.match(r\"\\[\\d\\d\\d\\d-(\\d\\d)-(\\d\\d) (\\d\\d):(\\d\\d)\\] (.*)\", line)\n",
    "    records.append((int(m.group(1)), int(m.group(2)), int(m.group(3)), int(m.group(4)), m.group(5)))\n",
    "\n",
    "# generate guard data\n",
    "records = sorted(records)\n",
    "guard_data = dict()  # dictionary mapping guard ID's (ints) to length-60 arrays of sleep frequencies\n",
    "curr_guard = -1\n",
    "sleeping = -1\n",
    "for record in records:\n",
    "    m = re.search(r\"\\d+\", record[4])\n",
    "    if m:  # \"Begin shift\" record\n",
    "        curr_guard = int(m.group(0))\n",
    "        if curr_guard not in guard_data:\n",
    "            guard_data[curr_guard] = [0] * 60\n",
    "    elif record[4] == 'falls asleep':  # \"falls asleep\" record\n",
    "        sleeping = record[3]\n",
    "    elif record[4] == 'wakes up':  # \"wakes up\" record\n",
    "        for i in range(sleeping, record[3]):\n",
    "            guard_data[curr_guard][i] += 1\n",
    "    else:\n",
    "        raise Exception('could not read data: {}'.format(record))\n",
    "        \n",
    "# process guard data\n",
    "best_id = ''\n",
    "best_total = 0\n",
    "best_minute = 0\n",
    "for guard in guard_data:\n",
    "    total = sum(guard_data[guard])\n",
    "    if total > best_total:\n",
    "        best_id = guard\n",
    "        best_total = total\n",
    "        best_minute = guard_data[guard].index(max(guard_data[guard]))\n",
    "\n",
    "best_id * best_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14920"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second star\n",
    "\n",
    "best_id = ''\n",
    "best_freq = 0\n",
    "best_minute = 0\n",
    "for guard in guard_data:\n",
    "    freq = max(guard_data[guard])\n",
    "    if freq > best_freq:\n",
    "        best_id = guard\n",
    "        best_freq = freq\n",
    "        best_minute = guard_data[guard].index(freq)\n",
    "\n",
    "best_id * best_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39698"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first star: revision\n",
    "\n",
    "with open('input/input4.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "def extract_entry(line):\n",
    "    m = re.match(r\"\\[\\d\\d\\d\\d-(\\d\\d)-(\\d\\d) (\\d\\d):(\\d\\d)\\] (.*)\", line)\n",
    "    return tuple(map(lambda i: int(m.group(i)), range(1, 5))) + (m.group(5),)\n",
    "\n",
    "records = sorted(map(extract_entry, lines))\n",
    "\n",
    "# generate guard data\n",
    "guard_data = defaultdict(lambda: [0] * 60)  # map guard ID's (ints) to length-60 arrays of sleep frequencies\n",
    "curr_guard = -1\n",
    "sleeping = -1\n",
    "for record in records:\n",
    "    m = re.search(r\"\\d+\", record[4])\n",
    "    if m:  # \"Begin shift\" record\n",
    "        curr_guard = int(m.group(0))\n",
    "    elif record[4] == 'falls asleep':  # \"falls asleep\" record\n",
    "        sleeping = record[3]\n",
    "    elif record[4] == 'wakes up':  # \"wakes up\" record\n",
    "        for i in range(sleeping, record[3]):\n",
    "            guard_data[curr_guard][i] += 1\n",
    "    else:\n",
    "        raise Exception('could not read data: {}'.format(record))\n",
    "\n",
    "best_total, best_minute, best_guard = max(map(\n",
    "    # tup[0] is the guard id, tup[1] is the guard's sleep times\n",
    "    lambda tup: (sum(tup[1]), tup[1].index(max(tup[1])), tup[0]),\n",
    "    guard_data.items()))\n",
    "\n",
    "best_guard * best_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14920"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second star: revision\n",
    "\n",
    "best_total, best_minute, best_guard = max(map(\n",
    "    # tup[0] is the guard id, tup[1] is the guard's sleep times\n",
    "    lambda tup: (max(tup[1]), tup[1].index(max(tup[1])), tup[0]),\n",
    "    guard_data.items()))\n",
    "\n",
    "best_guard * best_minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Day 5](https://adventofcode.com/2018/day/5) : Alchemical Reduction\n",
    "\n",
    "For the first star, my initial instinct was to try and somehow convert the string into a data structure which allowed constant time intermediate deletions, like a doubly-linked list. But as it happens, doubly-linked lists aren't natively available in Python, and creating a datastructure for this didn't seem like it would be very efficient (unless I was using a lower level language like C/C++/Rust perhaps).\n",
    "\n",
    "Fortunately, I realized I could accomplish the task easier if I just used a stack to continuously add elements to the \"result\" string, just checking if anything reacts with the top element in the stack. When one imagines the long chemical chain reacting in a vacuum, the first image that comes to my mind is a long line which slowly collapses inwards - but when you view it just from one side, the iterative approach clearly doesn't miss out on any reactions. This result can probably be shown easily using some form of proof by induction or contradiction.\n",
    "\n",
    "For the second star, I reused the same approach to simply iterate through the newly reduced string with a stack, but additionally checking if the character I am currently reading is a fixed target letter (e.g. 'a'), in which case I ignore it and move to the next character. I repeat this for every possible fixed letter, and keep track of what the lowest string length I obtained overall was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10564\n"
     ]
    }
   ],
   "source": [
    "# first star\n",
    "\n",
    "with open('input/input5.txt') as f:\n",
    "    X = f.readline().strip()\n",
    "\n",
    "stack = []\n",
    "for char in X:\n",
    "    if not stack:  # stack is empty\n",
    "        stack.append(char)\n",
    "    else:\n",
    "        e1 = stack[-1]\n",
    "        e2 = char\n",
    "        if e1.lower() == e2.lower() and \\\n",
    "                ((e1 == e1.lower() and e2 == e2.upper()) or \\\n",
    "                 (e1 == e1.upper() and e2 == e2.lower())):\n",
    "            stack.pop()\n",
    "        else:\n",
    "            stack.append(char)\n",
    "\n",
    "print(len(stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6336\n"
     ]
    }
   ],
   "source": [
    "# second star\n",
    "\n",
    "min_length = len(stack)\n",
    "for letter in string.ascii_lowercase:\n",
    "    new_stack = []\n",
    "    for char in stack:\n",
    "        if char == letter or char == letter.upper():\n",
    "            continue\n",
    "        elif not new_stack:\n",
    "            new_stack.append(char)\n",
    "        else:\n",
    "            e1 = new_stack[-1]\n",
    "            e2 = char\n",
    "            if e1.lower() == e2.lower() and \\\n",
    "                    ((e1 == e1.lower() and e2 == e2.upper()) or \\\n",
    "                     (e1 == e1.upper() and e2 == e2.lower())):\n",
    "                new_stack.pop()\n",
    "            else:\n",
    "                new_stack.append(char)\n",
    "    min_length = min(min_length, len(new_stack))\n",
    "\n",
    "print(min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10564"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first star: revision\n",
    "\n",
    "with open('input/input5.txt') as f:\n",
    "    X = f.readline().strip()\n",
    "\n",
    "def reduce_polymer(pol, exclude=None):\n",
    "    stack = []\n",
    "    for char in pol:\n",
    "        if exclude and (char == exclude or char == exclude.upper()):\n",
    "            continue\n",
    "        elif not stack:  # stack is empty\n",
    "            stack.append(char)\n",
    "        else:\n",
    "            top = stack[-1]\n",
    "            if top.lower() == char.lower() and top != char:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                stack.append(char)\n",
    "    return ''.join(stack) # converting back to string doesn't seem to impact performance\n",
    "\n",
    "first_reduction = reduce_polymer(X)\n",
    "len(first_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6336"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second star: revision\n",
    "\n",
    "min(map(lambda letter: len(reduce_polymer(first_reduction, letter)), string.ascii_lowercase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Day 6](https://adventofcode.com/2018/day/6): Chronal Coordinates\n",
    "\n",
    "This problem certainly off the bat seemed like a large difficulty spike compared to the previous puzzles. The problem was additionally challenging since I originally went down some incorrect paths, after intuitively thinking about the problem initially in terms of euclidaen distance, even though it actually specifies Manhattan distance to be used.\n",
    "\n",
    "Nonetheless, to start off this puzzle I began trying to think of how I could identify which points had areas that were finite. After some thinking (initially with the assumption of using Euclidean distance), I first realized that any point with finite area must be in some way \"enclosed\" by a polygon formed by other points, and moreover, there has to be a triangle of other points that enclose it. So if for each point I iterated over all triples of other points, and if I could easily calculate if one point was within a triangle of other points, then I could determine which points have bounded areas. (Calculating this is not trivial, but I found a StackOverfloow post offering an easy calculation for if a point is within a 2D triangle, and willingly borrowed it for my educational use).\n",
    "\n",
    "Later on, after realizing I had to use Manhattan distance, I had to reconsider this \"finite area\" identification subproblem, as the triangle-based solution no longer worked. One thing I noticed was that the diagrams formed by the points were very clearly Voronoi diagrams, as like the one shown in [this picture](https://commons.wikimedia.org/wiki/File:Manhattan_Voronoi_Diagram.svg). After enough studying of the picture, I concluded for a point to have finite area, then when considering the four diagonal quadrants surrounding the point (that is, visualized like the cartesian quadrants but at a 45 degree angle), there must be at least one point in each of the quadrants, assuming we label each point as the center of its four quadrants. So in a similar fashion as with the triangle idea, I iterated through all 4-combinations of points, and checked which would satisfy this four-quadrant constraint. (The exact checking method can be seen in the `point_in_diamond` function). Fortunately, as the input given only includes 50 points, 50^5 is just small enough number of iterations to be practical to handle all the combinations. Now, we have identified which of the 50 points will have finite areas.\n",
    "\n",
    "Also after inspecting the input, I noticed that all 50 of the input points were within roughly a 400x400 grid space. This makes for about 160,000 individual \"locations\" on the grid, for which it would not be that expensive to calculate which input point each location is closest to. From here, it's just a matter of summing the number of locations associated with each input point, filtering out the input points with infinite space, and taking the maximum out of all of them to get the answer!\n",
    "\n",
    "For the second star, the solution was easier as it just involve checking which points were within the constrained distance. Naturally, I just iterated over all of the locations, added the distance to each of the given points, and checked whether each was within the required constraint.\n",
    "\n",
    "Overall the code I wrote takes a while to run - around 30 seconds to a minute it seems, so I reckon there are probably some much more efficient algorithms for the problem. But I really enjoyed this challenge as a whole!\n",
    "\n",
    "Edit: Thanks to a kind suggestion of a friend, I was able to optimize the finite-area identification part of my algorithm from O(N^5) to O(N^2) (where N is the number of points). The trick is that we only actually need to identify the quadrant of each point once, to see if there is a point in the given quadrant. Thus, we do not actually have to know which four-tuple of points make up the quadrant. This allows me to calculate the solution to both parts within about 15 seconds now (with less than a second going towards the finite area calculation), so this is a substantial improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, False, True, True, False, False, False, True, False, False, False, True, False, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, False, False, False, True, True, True, False, False, False, False, False, True, False, True, True, True, True, True, False, True, True]\n",
      "3722\n",
      "44634\n"
     ]
    }
   ],
   "source": [
    "# test = \"\"\"1, 1\n",
    "# 1, 6\n",
    "# 8, 3\n",
    "# 3, 4\n",
    "# 5, 5\n",
    "# 8, 9\n",
    "# \"\"\"\n",
    "\n",
    "# X = test.split('\\n')[:-1]\n",
    "\n",
    "GRID_SIZE = 400\n",
    "\n",
    "with open('input/input6.txt') as f:\n",
    "    X = f.read().split('\\n')[:-1]  # remove last entry corresponding to empty line\n",
    "\n",
    "# points are stored as a list of coordinate tuples\n",
    "points = list(map(lambda x: tuple(map(int, x.split(', '))), X))\n",
    "\n",
    "def left_quad(x, test):\n",
    "    return test[0] < x[0] and abs(test[1] - x[1]) < (x[0] - test[0])\n",
    "\n",
    "def right_quad(x, test):\n",
    "    return test[0] > x[0] and abs(test[1] - x[1]) < (test[0] - x[0])\n",
    "\n",
    "def top_quad(x, test):\n",
    "    return test[1] > x[1] and abs(test[0] - x[0]) < (test[1] - x[1])\n",
    "\n",
    "def bottom_quad(x, test):\n",
    "    return test[1] < x[1] and abs(test[0] - x[0]) < (x[1] - test[1])\n",
    "\n",
    "# calculate which points have finite area\n",
    "finite = [False] * len(points)\n",
    "for i, point in enumerate(points):\n",
    "    l = r = t = b = False\n",
    "    for test_point in points:\n",
    "        if left_quad(point, test_point):\n",
    "            l = True\n",
    "        elif right_quad(point, test_point):\n",
    "            r = True\n",
    "        elif top_quad(point, test_point):\n",
    "            t = True\n",
    "        elif bottom_quad(point, test_point):\n",
    "            b = True\n",
    "    if l and r and t and b:\n",
    "        finite[i] = True\n",
    "\n",
    "# this is about the half way point of the calculation,\n",
    "# so its nice to get an update things are working!\n",
    "print(finite)\n",
    "\n",
    "def manhattan_dist(p0, p1):\n",
    "    return abs(p1[0] - p0[0]) + abs(p1[1] - p0[1])\n",
    "\n",
    "# calculate which point is closest for each location in the grid\n",
    "grid = [[-1] * GRID_SIZE for _ in range(GRID_SIZE)]\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        shortest_index = 0\n",
    "        shortest_dist = 2 * GRID_SIZE * GRID_SIZE\n",
    "        for k in range(len(points)):\n",
    "            dist = manhattan_dist(points[k], (i, j))\n",
    "            if dist < shortest_dist:\n",
    "                shortest_dist = dist\n",
    "                shortest_index = k\n",
    "            elif dist == shortest_dist:  # avoid ties\n",
    "                shortest_index = -1\n",
    "        grid[i][j] = shortest_index\n",
    "\n",
    "# calculate which point (with finite area) has the most locations closest to it\n",
    "flat_grid = [entry for row in grid for entry in row]\n",
    "best_point = 0\n",
    "best_count = 0\n",
    "for i in range(len(points)):\n",
    "    if not finite[i]:\n",
    "        continue\n",
    "    count = sum(map(lambda x: x == i, flat_grid))\n",
    "    if count > best_count:\n",
    "        best_count = count\n",
    "        best_point = i\n",
    "\n",
    "print(best_count)\n",
    "\n",
    "# second star\n",
    "\n",
    "TOTAL_DISTANCE = 10000\n",
    "\n",
    "# store a grid indicating which positions are \"centralized\",\n",
    "# i.e. within the total distance constraint\n",
    "central = [[False] * GRID_SIZE for _ in range(GRID_SIZE)]\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        total = 0\n",
    "        for k, point in enumerate(points):\n",
    "            total += manhattan_dist(points[k], (i, j))\n",
    "        if total < TOTAL_DISTANCE:\n",
    "            central[i][j] = True\n",
    "\n",
    "flat_central = [entry for row in central for entry in row]\n",
    "print(sum(flat_central))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Day 7](https://adventofcode.com/2018/day/7): The Sum of Its Parts\n",
    "\n",
    "The first star for this challenge was relatively straight forward, as I quickly identified that it just required a topological sort of the steps. For a minute or so I forgot how the algorithm worked (is it like a breadth first search starting from the root? or from the tail? or is it different?), but I've implemented it in Python before so I eventually recalled how it worked. The algorithm follows easily from the logic of the example they included. The only trick perhaps is getting items to be removed in alphabetical order to break ties, but this is easy with the built-in heapq module.\n",
    "\n",
    "The second star required just adding some additional components to handle the time-tracking logic separately. Essentially I just stored a dictionary of (letter, time-left) pairs to keep track of what is getting worked on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFKEGNOVATIHXYZRMCJDLSUPWQ\n",
      "BFKVEGAOTNYIHXZRMCJLDSUPWQ\n",
      "1020\n"
     ]
    }
   ],
   "source": [
    "# first star\n",
    "\n",
    "with open('input/input7.txt') as f:\n",
    "    X = f.readlines()\n",
    "\n",
    "X = list(map(lambda x: (x[5], x[36]), X))\n",
    "\n",
    "letters = set()\n",
    "reqs = defaultdict(set)  # map from a required step to the set of steps that depend on it\n",
    "rev_reqs = defaultdict(set)  # map from a step to the set of steps it requires\n",
    "for req_step, step in X:\n",
    "    reqs[req_step].add(step)\n",
    "    rev_reqs[step].add(req_step)\n",
    "    letters.add(step)\n",
    "    letters.add(req_step)\n",
    "\n",
    "ready = sorted([letter for letter in letters if len(rev_reqs[letter]) == 0])\n",
    "sequence = []\n",
    "while ready:\n",
    "    step = heappop(ready)\n",
    "    sequence.append(step)\n",
    "    for succ in reqs[step]:\n",
    "        rev_reqs[succ].remove(step)\n",
    "        if len(rev_reqs[succ]) == 0:\n",
    "            heappush(ready, succ)\n",
    "\n",
    "print(''.join(sequence))\n",
    "\n",
    "# second star\n",
    "\n",
    "# re-initialize variables\n",
    "sequence = []\n",
    "rev_reqs = defaultdict(set)\n",
    "for req_step, step in X:\n",
    "    rev_reqs[step].add(req_step)\n",
    "ready = sorted([letter for letter in letters if len(rev_reqs[letter]) == 0])\n",
    "\n",
    "times = {letter: 61 + ord(letter) - ord('A') for letter in letters}\n",
    "processing = dict()\n",
    "time = 0\n",
    "while ready or processing:\n",
    "    # more steps that workers can begin work on\n",
    "    while ready and len(processing) < 5:\n",
    "        step = heappop(ready)\n",
    "        processing[step] = times[step]\n",
    "        \n",
    "    # perform time-based updates\n",
    "    keys = list(processing.keys())\n",
    "    for key in keys:\n",
    "        processing[key] -= 1\n",
    "        if processing[key] == 0:\n",
    "            del processing[key]\n",
    "            sequence.append(key)\n",
    "            for succ in reqs[key]:\n",
    "                rev_reqs[succ].remove(key)\n",
    "                if len(rev_reqs[succ]) == 0:\n",
    "                    heappush(ready, succ)\n",
    "\n",
    "    time += 1\n",
    "    \n",
    "print(''.join(sequence))\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Day 8](https://adventofcode.com/2018/day/8): Memory Maneuver\n",
    "\n",
    "This challenge presented a simple but important design question, which is how should the problem solution be implemented. The direct calculation I had to make wasn't necessarily complicated, as it is simply an \"evaluation\" strategy for a particular tree structure (and the input string can easily be parsed into a tree via recursion and stacks), but how the implementation is done can be simple or complex.\n",
    "\n",
    "One totally viable strategy I was considering was one where I actually generated the tree corresponding to the input, and them performed a recursion computation according to it. In the first part, this option certainly tempted me since I knew there would be a second part of the problem, which surely would also require some kind of calculation based on trees. But in the end, I figured the problem would just be easier (and shorter) to implement by directly parsing the strings in a solution that combines iteration and recursion, so no tree data structure was needed. It's possible the solutions are less elegant, since there's no clear lines where the data transformations occur, and if you wanted to change the calculations in some way you have to essentially read through all of the manual iteration logic and figure out what line to change - but that said, most of the logic is chunked into \"stages\" of processing that should be easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48443, 18734)\n",
      "(30063, 18734)\n"
     ]
    }
   ],
   "source": [
    "# first star\n",
    "\n",
    "with open('input/input8.txt') as f:\n",
    "    X = list(map(int, f.read().split()))\n",
    "\n",
    "# X = list(map(int, \"2 3 0 3 10 11 12 1 1 0 1 99 2 1 1 2\".split()))\n",
    "\n",
    "# returns (sum, index) tuple where index is the next index to read\n",
    "def sum_entries(lst, index):\n",
    "    total = 0\n",
    "    num_children = lst[index]\n",
    "    num_entries = lst[index + 1]\n",
    "    index += 2\n",
    "    for _ in range(num_children):\n",
    "        subtotal, end = sum_entries(lst, index)\n",
    "        total += subtotal\n",
    "        index = end\n",
    "    total += sum(lst[index:index + num_entries])\n",
    "    return (total, index + num_entries)\n",
    "\n",
    "print(sum_entries(X, 0))\n",
    "\n",
    "# second star\n",
    "\n",
    "def node_value(lst, index):\n",
    "    num_children = lst[index]\n",
    "    num_entries = lst[index + 1]\n",
    "    index += 2\n",
    "    if num_children == 0:\n",
    "        value = sum(lst[index:index + num_entries])\n",
    "        return (value, index + num_entries)\n",
    "    children_values = []\n",
    "    for _ in range(num_children):\n",
    "        value, end = node_value(lst, index)\n",
    "        children_values.append(value)\n",
    "        index = end\n",
    "    value = 0\n",
    "    for i in range(num_entries):\n",
    "        entry = lst[index + i]\n",
    "        if entry > 0 and entry <= num_children:\n",
    "            value += children_values[entry - 1]\n",
    "    return (value, index + num_entries)\n",
    "\n",
    "print(node_value(X, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Day 9](https://adventofcode.com/2018/day/9): Marble Mania\n",
    "\n",
    "In principle, this problem is simple - the main challenge is finding a way to optimize it so that it runs quickly. Without any clear mathematical intuition for a quick shortcut for calculating scores, the core of my solution is to just simulate the entire marble game using a list. The cause for concern of course, is that we need to constantly be inserting to the list - and when our list gets large, this becomes very inefficient - particularly for the second star.\n",
    "\n",
    "One natural option would be to represent the circle as a circularly linked list, as by using pointers the insertion would be relatively constant -  but this isn't a natively implemented data structure in Python and I partially doubted how efficient it would be with the overhead of Python classes. To save time, I did a little bit of googling, and found a library named [blist](http://stutzbachenterprises.com/blist/blist.html) which offers lists with \n",
    "logarithmic insertion time, so I could improve the code while only having to change about two lines!\n",
    "\n",
    "In the end, after getting this working solution, I did write a solution with a doubly linked list implementation, and I'm happy with how it turned out. In the end the runtimes seem roughly comparable (both taking no more than 10-15 seconds), though I could imagine this could easily be much faster if it was implemented in C or another low level language. It's crazy actually having to write my own link list class after thinking it was just a data structure reserved for introductory data structure classes at college, but it was useful after all! I mostly just included methods for the `DLNode` (doubly-linked node) class that mattered for my problem, so it goes without saying a full implementation would probably have things interfacing slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422980\n",
      "422980\n",
      "3552041936\n",
      "3552041936\n"
     ]
    }
   ],
   "source": [
    "# first star\n",
    "\n",
    "with open('input/input9.txt') as f:\n",
    "    X = f.read().split(' ')\n",
    "    players = int(X[0])\n",
    "    marbles = int(X[6])\n",
    "\n",
    "def marble_score(num_players, num_marbles):\n",
    "    circle = blist([0])\n",
    "    curr_marble_index = 1\n",
    "    scores = [0] * num_players\n",
    "    for i in range(1, num_marbles + 1):\n",
    "        if i % 23 != 0:\n",
    "            new_index = (curr_marble_index + 2) % len(circle)\n",
    "            if new_index == 0:\n",
    "                circle.append(i)\n",
    "                curr_marble_index = len(circle) - 1\n",
    "            else:\n",
    "                circle.insert(new_index, i)\n",
    "                curr_marble_index = new_index\n",
    "        else:\n",
    "            curr_marble_index = (curr_marble_index - 7) % len(circle)\n",
    "            scores[(i - 1) % num_players] += i + circle[curr_marble_index]\n",
    "            circle.pop(curr_marble_index)\n",
    "    return max(scores)\n",
    "\n",
    "# doubly linked node\n",
    "class DLNode(object):\n",
    "    def __init__(self, val, nxt, prev):\n",
    "        self.val = val\n",
    "        self.next = nxt\n",
    "        self.prev = prev\n",
    "    \n",
    "    def insert_after(self, val):\n",
    "        new = DLNode(val, self.next, self)\n",
    "        self.next.prev = new\n",
    "        self.next = new\n",
    "    \n",
    "    def back(self, count):\n",
    "        curr = self\n",
    "        for _ in range(count):\n",
    "            curr = curr.prev\n",
    "        return curr\n",
    "    \n",
    "    # deletes self and returns the next node if there was any\n",
    "    def delete_self(self):\n",
    "        temp = self.next\n",
    "        if self.prev:\n",
    "            self.next.prev = self.prev\n",
    "        if self.next:\n",
    "            self.prev.next = temp\n",
    "        self.prev = None\n",
    "        self.next = None\n",
    "        return temp\n",
    "    \n",
    "    def __str__(self):\n",
    "        res = [str(self.val)]\n",
    "        curr = self\n",
    "        while curr.next and curr.next != self:\n",
    "            curr = curr.next\n",
    "            res.append(str(curr.val))\n",
    "        return '[{}]'.format(', '.join(res))\n",
    "\n",
    "def marble_score_ll(num_players, num_marbles):\n",
    "    curr = DLNode(0, None, None)\n",
    "    orig = curr\n",
    "    curr.next = curr\n",
    "    curr.prev = curr\n",
    "    scores = [0] * num_players\n",
    "    for i in range(1, num_marbles + 1):\n",
    "        if i % 23 != 0:\n",
    "            curr_next = curr.next\n",
    "            curr.next.insert_after(i)\n",
    "            curr = curr_next.next\n",
    "        else:\n",
    "            seven_back = curr.back(7)\n",
    "            scores[(i - 1) % num_players] += i + seven_back.val\n",
    "            seven_back_next = seven_back.delete_self()\n",
    "            curr = seven_back_next\n",
    "    return max(scores)\n",
    "            \n",
    "\n",
    "print(marble_score(players, marbles))\n",
    "print(marble_score_ll(players, marbles))\n",
    "\n",
    "# second star\n",
    "\n",
    "print(marble_score(players, marbles * 100))\n",
    "print(marble_score_ll(players, marbles * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## [Day 10](https://adventofcode.com/2018/day/10): The Stars Align\n",
    " \n",
    "It is late February 2019 and it has been a long time since I've attempted any of these challenges, but it never hurts to pick off another!\n",
    "\n",
    "This challenge wasn't too algorithmically difficult, as it was just a matter of being clever in how you find out when (and where) the points are going to converge. For me, the trick was looking at the data and noticing that despite the wide range of starting values for stars' locations, they all directly correspond to their initial velocities - so an x-coordinate in the 50000s has a starting x velocity of -5, and a y-coordinate in the -20000s has a starting y velocity of +2, etc. Thus I quickly extrapolated that I could just update the stars' positions by 10000 seconds straight in the future, and start guessing/simulating from there.\n",
    "\n",
    "I initially guessed that the points might end up in a small 100x100 range near the origin, though it turned out by both looking my image projections and the actual numerical ranges of the points, it was actually slightly further off, so I had to move my window a bit. It's definitely interesting to consider how one could generalize this task or do it more algorithmically (opposed to just using the code as a way to 'aid' the search process). One simply idea might be to just look for the point in time where the variance in the points' positions is smallest.\n",
    "\n",
    "I'm mostly satisfied with my solution, except the way I printed out a windowed region of the sky/graph seemed inelegant, as well as the way I parsed the values out of the input data in a somewhat hardcoded fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds:  10227\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "...........######..#....#....##....#.......#.......#....#..#.......#####............................\n",
      "...........#.......#...#....#..#...#.......#.......#...#...#.......#....#...........................\n",
      "...........#.......#..#....#....#..#.......#.......#..#....#.......#....#...........................\n",
      "...........#.......#.#.....#....#..#.......#.......#.#.....#.......#....#...........................\n",
      "...........#####...##......#....#..#.......#.......##......#.......#####............................\n",
      "...........#.......##......######..#.......#.......##......#.......#....#...........................\n",
      "...........#.......#.#.....#....#..#.......#.......#.#.....#.......#....#...........................\n",
      "...........#.......#..#....#....#..#.......#.......#..#....#.......#....#...........................\n",
      "...........#.......#...#...#....#..#.......#.......#...#...#.......#....#...........................\n",
      "...........######..#....#..#....#..######..######..#....#..######..#####............................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n"
     ]
    }
   ],
   "source": [
    "# both stars\n",
    "\n",
    "with open('input/input10.txt') as f:\n",
    "    X = f.readlines()\n",
    "\n",
    "class Star:\n",
    "    def __init__(self, pos, vel):\n",
    "        self.pos = pos\n",
    "        self.vel = vel\n",
    "    \n",
    "    def move(self, scale=1):\n",
    "        new_x = self.pos[0] + (self.vel[0] * scale)\n",
    "        new_y = self.pos[1] + (self.vel[1] * scale)\n",
    "        self.pos = (new_x, new_y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Star({}, {})'.format(self.pos[0], self.pos[1])\n",
    "\n",
    "POSX_INDEX = X[0].index('<') + 1  # 6 chars long\n",
    "POSY_INDEX = X[0].index(',') + 1  # 7 chars long\n",
    "VELX_INDEX = X[0].index('<', POSX_INDEX + 1) + 1  # 2 chars long\n",
    "VELY_INDEX = X[0].index(',', POSY_INDEX + 1) + 1  # 3 chars long\n",
    "\n",
    "def create_star(line):\n",
    "    posx = int(line[POSX_INDEX:POSX_INDEX + 6])\n",
    "    posy = int(line[POSY_INDEX:POSY_INDEX + 7])\n",
    "    velx = int(line[VELX_INDEX:VELX_INDEX + 2])\n",
    "    vely = int(line[VELY_INDEX:VELY_INDEX + 3])\n",
    "    return Star((posx, posy), (velx, vely))\n",
    "\n",
    "# randomly guessing that everything will appear within x and y ranges of [100, 200]\n",
    "\n",
    "WINDOW_LEFT = 100\n",
    "WINDOW_SIZE = 100\n",
    "\n",
    "def print_map(stars):\n",
    "    grid = [['.'] * WINDOW_SIZE for _ in range(WINDOW_SIZE)]\n",
    "    for star in stars:\n",
    "        if star.pos[0] >= WINDOW_LEFT and star.pos[0] < WINDOW_LEFT + WINDOW_SIZE \\\n",
    "            and star.pos[1] >= WINDOW_LEFT and star.pos[1] < WINDOW_LEFT + WINDOW_SIZE:\n",
    "            grid[star.pos[1] - WINDOW_LEFT][star.pos[0] - WINDOW_LEFT] = '#'\n",
    "    for row in grid:\n",
    "        print(''.join(row))\n",
    "\n",
    "def update_stars(stars, scale=1):\n",
    "    for star in stars:\n",
    "        star.move(scale)\n",
    "\n",
    "data = list(map(create_star, X[:-1]))\n",
    "print('String: ')\n",
    "print('Seconds: ', 10227)\n",
    "update_stars(data, 10227)\n",
    "print_map(data)\n",
    "\n",
    "# # show the points converging\n",
    "# iters = 10225\n",
    "# update_stars(data, iters) # starting point\n",
    "# for _ in range(10):\n",
    "#     update_stars(data, 1)\n",
    "#     iters += 1\n",
    "#     print('Iters: ', iters)\n",
    "#     print_map(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 10.5: Intermission\n",
    "\n",
    "At this point, I've solved 10 of the problems. While I'm a way from finishing, I'd like to pause to take a moment to look back at my solutions, review my code, and look for ways to improve efficiency or improve modularity. After this, I'll also be taking a look at [Peter Norvig's](https://github.com/norvig/pytudes/blob/master/ipynb/Advent-2018.ipynb) solutions to see how another experienced Python user approaches the problems, and what I can learn from it.\n",
    "\n",
    "### Day 1\n",
    "\n",
    "For the second star, I managed to find a way to make the solution slightly more functional by performing the repeat-checking in a helper function and then just using `filter`. But altogether this doesn't make a large difference. In Norvig's code, it's interesting that his `partial_sums` function is just a rewrite of accumulate I believe, so this is slightly redundant. Otherwise the solutions are comparable.\n",
    "\n",
    "### Day 2\n",
    "\n",
    "For both of the stars in this problem, I felt fairly confident with my solution. That said, I admire Norvig's use of a `quantify` function for the first star, which makes things looks even cleaner. I believe my solution may still be more efficient since I only end up evaluating `Counter(x)` once per ID (whereas I have doubts as to whether the compiler would optimize his code to do so). Also, even though I was proud of my one-liners for the second star, it turns out using the `zip` function yields an even cleaner result. Likewise I simplified my nested for-loops into a one-liner by using `combinations`, which is slightly more efficient than Norvig's two-part list comprehension (which may check many pairs of boxes twice).\n",
    "\n",
    "### Day 3\n",
    "\n",
    "For this problem, I initially named my variables quite poorly (using letters a, b, c, d, e instead of claim, x, y, w, h) so I just modified this in place to improve clarity. Then, for the second star I came up with a simpler check for claims which simply generated an list of the tiles in the appropriate rectangle, and performed a boolean operation over it to check if any are False. I'd worry that this solution could be slower if `all` operation doesn't short circuit at the first occurrence of a false value, but since `all` and `map` both operate in terms of iterables, it should be optimized by default.\n",
    "\n",
    "That said, Norvig's improvements still impress me. It's extremely nice how he is able to parse input (just extracting the relevant integers) using his `Input` function, so perhaps I will adapt this at some point. His solution for counting grid cells is also rather clean. For the second star our solutions seem comparable, but I do like his first star solution (using Counter) a lot better given that it requires no if statements or for loops. That said, I reckon `Counter` might be less space efficient than storing the grid directly as a 2D list.\n",
    "\n",
    "### Day 4\n",
    "\n",
    "For this problem my solution overall seems pretty long and clunky (both in terms of parsing the input and then calculating the input data), but it was hard for me to find clear optimizations. As usual, Norvig has a clean way to parse the regular expression - first he performs substitutions from symbols to spaces, and then uses text.split(), allowing him to easily unpack the expressions. Also, he parses the entire date as a single unit, which is quite reasonable. The rest of the solution is also extremely clear to follow, using a similar functional style as usual. His solution to part 2 requires some thought as to how his code counts the minutes, but overall is still very understandable. Both dividing pieces of code into functions and using string substitution for parsing are both techniques I should use more often.\n",
    "\n",
    "### Day 5\n",
    "\n",
    "Here I was able to make various minor improvements, including making the reaction checking simpler, and reducing the second star down to just a single line by making the first function more modular. That said, I'm not sure if there is an easy way to make my core `reduce_polymer` function any more simpler, since the algorithm I am applying (which relies on pushing and popping elements on a stack) doesn't seem to lend itself easily to simple map operations and such. As I look at Norvig's solution, I seem to have confirmed this - although he uses a simple regex-substition loop to remove the characters, which is much easier to write albeit much less efficient. I think this is part of the beauty of Python though - the code takes so little time to write that if it ends up being too inefficient for our purposes, the time we spent writing the initial solution is overall negligible. Also, the fact that he even defines a \"shortest\" function just to improve readability is a nice touch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
